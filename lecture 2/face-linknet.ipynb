{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":847360,"sourceType":"datasetVersion","datasetId":177014},{"sourceId":10219737,"sourceType":"datasetVersion","datasetId":6317594}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install facenet_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:54:48.706433Z","iopub.execute_input":"2024-12-16T20:54:48.706780Z","iopub.status.idle":"2024-12-16T20:57:14.773476Z","shell.execute_reply.started":"2024-12-16T20:54:48.706747Z","shell.execute_reply":"2024-12-16T20:57:14.772647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade Pillow\nimport os\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport albumentations as A\nfrom facenet_pytorch import MTCNN\nfrom torchvision import transforms\nfrom torchvision.models.segmentation import deeplabv3_resnet101\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:14.775603Z","iopub.execute_input":"2024-12-16T20:57:14.776282Z","iopub.status.idle":"2024-12-16T20:57:28.413957Z","shell.execute_reply.started":"2024-12-16T20:57:14.776238Z","shell.execute_reply":"2024-12-16T20:57:28.413232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmtcnn = MTCNN(keep_all=True, device=device)\nmtcnn.eval()\nmodel = deeplabv3_resnet101(pretrained=True).to(device)\nmodel.eval()\n\ndef process_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    img = cv2.imread(image_path)\n\n    boxes, _ = mtcnn.detect(image)\n    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n\n    if boxes is not None:\n        for box in boxes:\n            x1, y1, x2, y2 = map(int, box)\n            mask[y1:y2, x1:x2] = 255 \n\n    black_background = img.copy()\n    black_background[mask == 0] = [0, 0, 0]\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((512, 512)),\n    ])\n\n    input_tensor = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model(input_tensor)['out'][0]\n    output_predictions = torch.argmax(output, dim=0).byte().cpu().numpy()\n\n    face_mask = (output_predictions == 15).astype(np.uint8)\n\n    if face_mask.shape != img.shape[:2]:\n        face_mask = cv2.resize(face_mask, (img.shape[1], img.shape[0])) \n\n    face_mask_binary = (face_mask > 0).astype(np.uint8)\n    result_refined = cv2.bitwise_and(img, img, mask=face_mask_binary)\n    \n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB), cv2.cvtColor(result_refined, cv2.COLOR_BGR2RGB), face_mask_binary\n\ndef display_results(original, refined, mask):\n    \"\"\"\n    Показывает три изображения в строке: исходное, обрезанное и маску.\n\n    Args:\n        original (ndarray): Исходное изображение.\n        refined (ndarray): Обрезанное изображение с черным фоном.\n        mask (ndarray): Маска (0, 1).\n    \"\"\"\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(original)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(refined)\n    plt.title(\"Refined Image\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(mask, cmap='gray')\n    plt.title(\"Mask\")\n    plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:28.415328Z","iopub.execute_input":"2024-12-16T20:57:28.415683Z","iopub.status.idle":"2024-12-16T20:57:31.409758Z","shell.execute_reply.started":"2024-12-16T20:57:28.415656Z","shell.execute_reply":"2024-12-16T20:57:31.408819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, result_refined, face_mask_binary = process_image('/kaggle/input/imdb-wiki-faces-dataset/imdb_crop/07/nm0000107_rm1223996160_1953-12-8_1987.jpg')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:31.412107Z","iopub.execute_input":"2024-12-16T20:57:31.412879Z","iopub.status.idle":"2024-12-16T20:57:31.989727Z","shell.execute_reply.started":"2024-12-16T20:57:31.412835Z","shell.execute_reply":"2024-12-16T20:57:31.988762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_results(img, result_refined, face_mask_binary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:31.990773Z","iopub.execute_input":"2024-12-16T20:57:31.991043Z","iopub.status.idle":"2024-12-16T20:57:32.561639Z","shell.execute_reply.started":"2024-12-16T20:57:31.991018Z","shell.execute_reply":"2024-12-16T20:57:32.560813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"!pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:32.562969Z","iopub.execute_input":"2024-12-16T20:57:32.563222Z","iopub.status.idle":"2024-12-16T20:57:41.477867Z","shell.execute_reply.started":"2024-12-16T20:57:32.563196Z","shell.execute_reply":"2024-12-16T20:57:41.477003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_paths(root_path):\n    \"\"\"\n    Обходит все директории внутри заданного пути и возвращает список путей ко всем изображениям.\n\n    Args:\n        root_path (str): Корневая директория.\n\n    Returns:\n        np.ndarray: Массив путей ко всем изображениям.\n    \"\"\"\n    images_paths = []\n\n    for dirpath, _, filenames in os.walk(root_path):\n        for filename in sorted(filenames):\n            if filename.lower().endswith(('.jpg', '.jpeg')):\n                images_paths.append(os.path.join(dirpath, filename))\n\n    return np.stack(images_paths)\n\n\nclass FaceDataset(Dataset):\n    def __init__(self, images, transform, augmentations, padded=False, normalize=False):\n        self.image_paths = sorted(images) \n        self.transform = transform \n        self.augmentations = augmentations\n        self.padded = padded\n        self.normalize = None\n        if normalize:\n            self.normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        im_path = self.image_paths[idx]\n        image, _, mask = process_image(im_path) \n        image = image / 255.0 \n        mask = (mask > 0).astype(np.float32) \n    \n        augmentation = np.random.choice(self.augmentations)\n        if augmentation is None:\n            augmented_image = image\n            augmented_mask = mask\n        else:\n            augmented = augmentation(image=image, mask=mask)\n            augmented_image = augmented['image']\n            augmented_mask = augmented['mask']\n    \n        augmented_image = self.transform(augmented_image).float()\n        augmented_mask = self.transform(augmented_mask).float()\n        \n        if self.normalize:\n            augmented_image = self.normalize(augmented_image)\n    \n        return {\n            'image': augmented_image,  \n            'mask': augmented_mask  \n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:41.479366Z","iopub.execute_input":"2024-12-16T20:57:41.479715Z","iopub.status.idle":"2024-12-16T20:57:41.489232Z","shell.execute_reply.started":"2024-12-16T20:57:41.479677Z","shell.execute_reply":"2024-12-16T20:57:41.488382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import scipy\nimport pandas as pd \nfrom datetime import datetime, timedelta\nimport bz2\nmat_data = scipy.io.loadmat('/kaggle/input/imdb-info/imdb.mat')\ndt = mat_data['imdb'][0, 0]\n\nkeys_s = ('gender', 'dob', 'photo_taken',\n          'face_score', 'second_face_score', 'celeb_id')\nvalues = {k: dt[k].squeeze() for k in keys_s}\n\nkeys_n = ('full_path', 'name')\nfor k in keys_n:\n    values[k] = np.array([x if not x else x[0] for x in dt[k][0]])\n\n# img(face_location(2):face_location(4),face_location(1):face_location(3),:))\nvalues['face_location'] =\\\n    [tuple(x[0].tolist()) for x in dt['face_location'].squeeze()]\n\nset_nrows = {len(v) for _, v in values.items()}\nassert len(set_nrows) == 1\n\ndf_values = pd.DataFrame(values)\n\nmatlab_origin = datetime(1, 1, 1)  # MATLAB starts on January 1, year 1\ndays_offset = timedelta(days=366)\n\ndef matlab_datenum_to_datetime(datenum):\n    try:\n        if datenum > 0 and datenum < 3652059:  # 3652059 is approximately the year 9999 in MATLAB\n            return matlab_origin + timedelta(days=datenum) - days_offset\n        else:\n            return pd.NaT \n    except OverflowError:\n        return pd.NaT\n\ndf_values['dob'] = df_values['dob'].apply(matlab_datenum_to_datetime)\nnames = dt['celeb_names']\nlen([str(item[0]) for sublist in names for item in sublist])\nlen(np.unique(df_values['name'].values))\n# Шаг 2: Фильтрация данных\nfiltered_df = df_values[\n    (df_values['face_score'] > 0) & \n    (df_values['second_face_score'].isna())  # Убираем изображения с несколькими лицами\n]\n\nbest_images = (\n    filtered_df.sort_values(by=['face_score', 'photo_taken'], ascending=[False, False])\n    .groupby('celeb_id')\n    .first()\n    .reset_index()\n)\nbest_images = best_images.drop(columns=['second_face_score', 'celeb_id'])\nimage_paths = [\"/kaggle/input/imdb-wiki-faces-dataset/imdb_crop/\" + path for path in best_images['full_path']]\nprint(len(image_paths))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:41.490285Z","iopub.execute_input":"2024-12-16T20:57:41.490649Z","iopub.status.idle":"2024-12-16T20:57:49.633854Z","shell.execute_reply.started":"2024-12-16T20:57:41.490616Z","shell.execute_reply":"2024-12-16T20:57:49.632952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_train_test(data, test_size=0.2, random_state=42):\n    if random_state:\n        np.random.seed(random_state)\n    indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_size)\n    test_indices = indices[:test_set_size]\n    train_indices = indices[test_set_size:]\n    return data[train_indices], data[test_indices]\n\n\nX_train, X_test = split_train_test(np.array(image_paths), test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:49.635079Z","iopub.execute_input":"2024-12-16T20:57:49.635938Z","iopub.status.idle":"2024-12-16T20:57:49.649364Z","shell.execute_reply.started":"2024-12-16T20:57:49.635891Z","shell.execute_reply":"2024-12-16T20:57:49.648546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(), transforms.Resize((512, 512))])\n\naugmentation = [\n            None,\n            A.HorizontalFlip(p=1),        \n            A.Rotate(limit=5, p=1),                                        \n            A.VerticalFlip(p=1),                        \n            A.RandomBrightnessContrast(brightness_limit=1, contrast_limit=1, p=1.0)]    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:49.653042Z","iopub.execute_input":"2024-12-16T20:57:49.653315Z","iopub.status.idle":"2024-12-16T20:57:49.709883Z","shell.execute_reply.started":"2024-12-16T20:57:49.653291Z","shell.execute_reply":"2024-12-16T20:57:49.708942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = FaceDataset(\n    images=X_train,\n    transform=transform,\n    augmentations=augmentation,\n    padded=True, normalize=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:49.711139Z","iopub.execute_input":"2024-12-16T20:57:49.711455Z","iopub.status.idle":"2024-12-16T20:57:49.727686Z","shell.execute_reply.started":"2024-12-16T20:57:49.711430Z","shell.execute_reply":"2024-12-16T20:57:49.726783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_example(dataset, idx):\n    sample = dataset[idx]\n    image = sample['image']\n    mask = sample['mask']\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(image.permute(1, 2, 0) * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n    plt.axis('off') \n    plt.subplot(1, 2, 2)\n    plt.imshow(mask.permute(1, 2, 0), cmap='gray')\n    plt.axis('off') \n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:49.728861Z","iopub.execute_input":"2024-12-16T20:57:49.729078Z","iopub.status.idle":"2024-12-16T20:57:49.738957Z","shell.execute_reply.started":"2024-12-16T20:57:49.729057Z","shell.execute_reply":"2024-12-16T20:57:49.738079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_example(dataset, 700)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:49.740143Z","iopub.execute_input":"2024-12-16T20:57:49.740762Z","iopub.status.idle":"2024-12-16T20:57:50.132940Z","shell.execute_reply.started":"2024-12-16T20:57:49.740712Z","shell.execute_reply":"2024-12-16T20:57:50.131074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Dataset length {len(dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.134704Z","iopub.execute_input":"2024-12-16T20:57:50.136761Z","iopub.status.idle":"2024-12-16T20:57:50.146026Z","shell.execute_reply.started":"2024-12-16T20:57:50.136705Z","shell.execute_reply":"2024-12-16T20:57:50.144253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calc_iou(prediction, ground_truth):\n    n_images = len(prediction)\n    intersection, union = 0, 0\n    for i in range(n_images):\n        intersection += np.logical_and(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum()\n        union += np.logical_or(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum()\n    return float(intersection) / union\n\ndef dice_loss(target, prediction, eps=1e-8):\n    prediction = torch.sigmoid(prediction)\n    intersection = torch.sum(target * prediction)\n    total = torch.sum(target) + torch.sum(prediction)\n    \n    loss = 1 - (2 * intersection + eps) / (total + eps)\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.147866Z","iopub.execute_input":"2024-12-16T20:57:50.149410Z","iopub.status.idle":"2024-12-16T20:57:50.157813Z","shell.execute_reply.started":"2024-12-16T20:57:50.149359Z","shell.execute_reply":"2024-12-16T20:57:50.156854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LinkNet architecture:\nIn LinkNet, the input of each encoder layer is also passed to the output of its corresponding decoder. See [paper](https://www.theobjects.com/dragonfly/dfhelp/Content/Resources/PDFs/linknet.pdf).\n\n<img src=\"https://wiki.cloudfactory.com/media/pages/docs/mp-wiki/model-architectures/linknet/b19f6073bd-1684131960/linknet.webp\">","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=False):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, groups=groups, bias=bias),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(out_channels)\n        ) if stride > 1 else nn.Identity()\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv2(self.conv1(x))\n        return nn.ReLU(inplace=True)(out + residual)\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=False):\n        super().__init__()\n        self.blocks = nn.Sequential(\n            EncoderBlock(in_channels, out_channels, kernel_size, stride, padding, groups, bias),\n            EncoderBlock(out_channels, out_channels, kernel_size, 1, padding, groups, bias)\n        )\n\n    def forward(self, x):\n        return self.blocks(x)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=False):\n        super().__init__()\n        mid_channels = in_channels // 4\n        \n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(mid_channels, mid_channels, kernel_size, stride, padding, output_padding, bias=bias),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n    \n    \nclass Classifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.tp_conv1 = self._conv_transpose_block(64, 32)\n        self.conv2 = self._conv_block(32)\n        self.tp_conv2 = nn.ConvTranspose2d(32, n_classes, kernel_size=2, stride=2)\n\n    def _conv_block(self, in_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def _conv_transpose_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.tp_conv2(self.conv2(self.tp_conv1(x)))\n    \nclass LinkNet(nn.Module):\n    def __init__(self, n_classes=1, pretrained=False):\n        super().__init__()\n        if pretrained:\n            base = resnet.resnet18(pretrained=True)\n\n            self.in_block = nn.Sequential(\n                base.conv1,\n                base.bn1,\n                base.relu,\n                base.maxpool\n            )\n\n            self.encoders = nn.ModuleList([base.layer1,\n                                           base.layer2,\n                                           base.layer3,\n                                           base.layer4])\n        else:\n            self.in_block = nn.Sequential(\n                nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(3, 2, 1)\n            )\n            \n            encoder_params = [\n                (64, 64, 3, 1, 1),\n                (64, 128, 3, 2, 1),\n                (128, 256, 3, 2, 1),\n                (256, 512, 3, 2, 1)\n            ]\n\n            self.encoders = nn.ModuleList([Encoder(*params) for params in encoder_params])\n        \n        decoder_params = [\n            (64, 64, 3, 1, 1, 0),\n            (128, 64, 3, 2, 1, 1),\n            (256, 128, 3, 2, 1, 1),\n            (512, 256, 3, 2, 1, 1)\n        ]\n        \n        self.decoders = nn.ModuleList([Decoder(*params) for params in decoder_params])\n\n\n        # Classifier\n        self.final = Classifier(n_classes)\n\n\n    def forward(self, x):\n        # Initial block\n        x = self.in_block(x)\n\n        # Encoder blocks\n        e1 = self.encoders[0](x)\n        e2 = self.encoders[1](e1)\n        e3 = self.encoders[2](e2)\n        e4 = self.encoders[3](e3)\n\n        # Decoder blocks\n        d4 = e3 + self.decoders[3](e4)\n        d3 = e2 + self.decoders[2](d4)\n        d2 = e1 + self.decoders[1](d3)\n        d1 = x + self.decoders[0](d2)\n\n        return self.final(d1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.159173Z","iopub.execute_input":"2024-12-16T20:57:50.159420Z","iopub.status.idle":"2024-12-16T20:57:50.365635Z","shell.execute_reply.started":"2024-12-16T20:57:50.159396Z","shell.execute_reply":"2024-12-16T20:57:50.364675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = FaceDataset(\n    images=X_test,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Resize((512, 512))]), \n    augmentations=[None],\n    padded=True, normalize=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.366804Z","iopub.execute_input":"2024-12-16T20:57:50.367078Z","iopub.status.idle":"2024-12-16T20:57:50.381555Z","shell.execute_reply.started":"2024-12-16T20:57:50.367053Z","shell.execute_reply":"2024-12-16T20:57:50.380646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef validate_model(model, val_loader):\n    model.eval()\n    val_loss = 0\n    ious = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            images = batch[\"image\"].to(device)\n            masks = batch[\"mask\"].to(device)\n            \n            outputs = model(images)\n            loss = dice_loss(masks, outputs)\n            val_loss += loss.item()\n            \n            iou = calc_iou(outputs.to('cpu').detach().numpy(), masks.to('cpu').numpy())\n            ious.append(iou)\n    \n    avg_val_loss = val_loss / len(val_loader)\n    avg_iou = sum(ious) / len(ious)\n    \n    return {\n            \"val_loss\": avg_val_loss,\n            \"val_iou\": avg_iou\n            }\n\n\n\ndef train_model(model, train_loader, val_loader, optimizer, sheduler, n_epochs):\n    model.to(device)\n    \n    metrics = {\n        \"train_loss\": [],\n        \"train_iou\": [],\n        \"val_loss\": [],\n        \"val_iou\": []\n    }\n    \n    for epoch in tqdm(range(n_epochs)):\n        model.train()\n        train_loss = 0\n        ious = []\n        for batch in train_loader:\n            optimizer.zero_grad()\n            images = batch[\"image\"].to(device)\n            masks = batch[\"mask\"].to(device)\n            \n            outputs = model(images)\n            loss = dice_loss(masks, outputs)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            iou = calc_iou(outputs.to('cpu').detach().numpy(), masks.to('cpu').numpy())\n            ious.append(iou)\n        scheduler.step()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_train_iou = sum(ious) / len(ious)\n        \n        val_metrics = validate_model(model, val_loader)\n        \n        metrics[\"train_loss\"].append(avg_train_loss)\n        metrics[\"train_iou\"].append(avg_train_iou)\n        metrics[\"val_loss\"].append(val_metrics[\"val_loss\"])\n        metrics[\"val_iou\"].append(val_metrics[\"val_iou\"])\n        \n\n        print(f'Epoch [{epoch+1}/{n_epochs}], '\n              f'Train Loss: {avg_train_loss:.4f}, '\n              f'Train IoU: {avg_train_iou:.4f}, '\n              f'Val Loss: {val_metrics[\"val_loss\"]:.4f}, '\n              f'Val IoU: {val_metrics[\"val_iou\"]:.4f}')\n    \n    return model, metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.382675Z","iopub.execute_input":"2024-12-16T20:57:50.382951Z","iopub.status.idle":"2024-12-16T20:57:50.393557Z","shell.execute_reply.started":"2024-12-16T20:57:50.382926Z","shell.execute_reply":"2024-12-16T20:57:50.392799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loader = DataLoader(\n    test_dataset,           \n    batch_size=16,\n    shuffle=False,\n)\n\ntrain_loader = DataLoader(\n    dataset,           \n    batch_size=16,\n    shuffle=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.394848Z","iopub.execute_input":"2024-12-16T20:57:50.395189Z","iopub.status.idle":"2024-12-16T20:57:50.407477Z","shell.execute_reply.started":"2024-12-16T20:57:50.395154Z","shell.execute_reply":"2024-12-16T20:57:50.406642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_linknet = LinkNet()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.8)\ntrained_model, metrics = train_model(model_linknet, train_loader, val_loader, optimizer, scheduler, n_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:57:50.408533Z","iopub.execute_input":"2024-12-16T20:57:50.408903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(metrics, baseline=0.75):\n    plt.figure(figsize=(10, 5))\n    plt.plot(metrics[\"train_loss\"], label='Train Loss')\n    plt.plot(metrics[\"val_loss\"], label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Dice Loss for scratch LinkNet')\n    plt.grid()\n    plt.show()\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(metrics[\"val_iou\"], label='Validation IoU', color='blue')\n    plt.plot(metrics[\"train_iou\"], label='Train IoU', color='orange')\n\n    max_iou_idx = np.argmax(metrics[\"val_iou\"])\n    max_iou_value = metrics[\"val_iou\"][max_iou_idx]\n    plt.scatter(max_iou_idx, max_iou_value, color='green', zorder=5) \n    plt.text(max_iou_idx, max_iou_value, f'Max IoU: {max_iou_value:.2f}', \n             verticalalignment='bottom', horizontalalignment='right')\n\n    plt.axhline(y=baseline, color='red', linestyle='--', label='Baseline IoU')\n    plt.xlabel('Epochs')\n    plt.ylabel('IoU')\n    plt.legend()\n    plt.title('IoU for scratch LinkNet')\n    plt.grid()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:58.986237Z","iopub.status.idle":"2024-12-16T19:46:58.986753Z","shell.execute_reply.started":"2024-12-16T19:46:58.986478Z","shell.execute_reply":"2024-12-16T19:46:58.986503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:58.988183Z","iopub.status.idle":"2024-12-16T19:46:58.988950Z","shell.execute_reply.started":"2024-12-16T19:46:58.988411Z","shell.execute_reply":"2024-12-16T19:46:58.988444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_linknet = LinkNet(pretrained=True)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.8)\ntrained_model, metrics = train_model(model_linknet, train_loader, val_loader, optimizer, scheduler, n_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:58.990944Z","iopub.status.idle":"2024-12-16T19:46:58.991436Z","shell.execute_reply.started":"2024-12-16T19:46:58.991180Z","shell.execute_reply":"2024-12-16T19:46:58.991204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:58.993312Z","iopub.status.idle":"2024-12-16T19:46:58.993834Z","shell.execute_reply.started":"2024-12-16T19:46:58.993559Z","shell.execute_reply":"2024-12-16T19:46:58.993604Z"}},"outputs":[],"execution_count":null}]}